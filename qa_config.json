{
    "======== MODEL ARGS ========": null,
    "pretrained_name": "hfl/chinese-macbert-large",
    "tokenizer_name": "hfl/chinese-macbert-large",
    "cache_dir": null,
    "ckpt_path": null,
    "from_scratch": false,
    "bert_config": {
        "attention_probs_dropout_prob": 0.1,
        "classifier_dropout": null,
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.1,
        "hidden_size": 768,
        "initializer_range": 0.02,
        "intermediate_size": 3072,
        "layer_norm_eps": 1e-12,
        "max_position_embeddings": 512,
        "num_hidden_layers": 12,
        "num_attention_heads": 12,
        "pad_token_id": 0,
        "position_embedding_type": "absolute",
        "type_vocab_size": 2,
        "use_cache": true,
        "vocab_size": 21128
    },
    "======== DATA ARGS ========": null,
    "context_file": "./data/context.json",
    "train_file": "./data/train.json",
    "valid_file": "./data/valid.json",
    "test_file": "./data/test.json",
    "max_len": 384,
    "pad_to_max_len": true,
    "doc_stride": 128,
    "======== TRAINING ARGS ========": null,
    "seed": 777,
    "fp16": true,
    "num_train_epochs": 5,
    "learning_rate": 5e-5,
    "weight_decay": 0,
    "per_device_train_batch_size": 2,
    "per_valid_train_batch_size": 8,
    "gradient_accumulation_steps": 4,
    "output_dir": "./qa_output",
    "evaluation_strategy": "steps",
    "eval_steps": 1000,
    "logging_strategy": "steps",
    "logging_steps": 200,
    "logging_dir": null,
    "save_strategy": "epoch",
    "save_total_limit": 3
}